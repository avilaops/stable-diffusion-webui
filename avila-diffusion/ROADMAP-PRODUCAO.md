# üéØ ROADMAP - Do Prot√≥tipo √† Produ√ß√£o Fotoreal√≠stica

## üìä STATUS ATUAL (v4.0)

### ‚úÖ J√Å TEMOS (100% Funcional):
- [x] Arquitetura completa implementada
- [x] Carregamento de modelos SafeTensors (2-6GB)
- [x] 8 modelos prontos para uso
- [x] Servidor HTTP nativo
- [x] Frontend moderno (dark/light theme)
- [x] API REST completa
- [x] Gerador procedural (fallback)
- [x] Pipeline text-to-image estruturado
- [x] Compila√ß√£o r√°pida (7-10 segundos)

### ‚ö†Ô∏è SIMPLIFICADO (Precisa Upgrade):
- [ ] UNet - Forward pass n√£o usa weights reais
- [ ] VAE - Apenas downsampling/upsampling b√°sico
- [ ] CLIP - Hash de palavras em vez de transformer
- [ ] Schedulers - Implementa√ß√£o b√°sica funciona, mas sem otimiza√ß√µes

---

## üöÄ N√çVEIS DE EVOLU√á√ÉO

### N√çVEL 1: Prot√≥tipo Funcional ‚úÖ ATUAL
**Status**: COMPLETO
**Capacidade**: Gera√ß√£o procedural r√°pida
**Tempo de gera√ß√£o**: < 1 segundo
**Qualidade**: Padr√µes abstratos/cores baseadas em prompt

**O que funciona**:
- Interface completa
- API REST
- An√°lise de prompts
- Output em PNG/Base64

---

### N√çVEL 2: H√≠brido (Python + Rust) üéØ RECOMENDADO
**Status**: N√ÉO INICIADO
**Esfor√ßo**: 2-3 dias
**Capacidade**: Gera√ß√£o fotoreal√≠stica usando PyTorch

#### O que precisa:

1. **Integrar PyTorch via PyO3** (1 dia)
```rust
use pyo3::prelude::*;
use pyo3::types::PyDict;

pub struct HybridUNet {
    python_model: Py<PyAny>,
}

impl HybridUNet {
    pub fn load(model_path: &str) -> Result<Self, String> {
        Python::with_gil(|py| {
            // Importar diffusers do HuggingFace
            let diffusers = py.import("diffusers")?;
            let torch = py.import("torch")?;

            // Carregar pipeline
            let pipeline = diffusers
                .getattr("StableDiffusionPipeline")?
                .call_method1("from_single_file", (model_path,))?;

            Ok(Self {
                python_model: pipeline.into(),
            })
        })
    }

    pub fn forward(&self, prompt: &str, steps: i32) -> Result<Vec<u8>, String> {
        Python::with_gil(|py| {
            let kwargs = PyDict::new(py);
            kwargs.set_item("prompt", prompt)?;
            kwargs.set_item("num_inference_steps", steps)?;

            let result = self.python_model
                .call_method(py, "__call__", (), Some(kwargs))?;

            // Extrair imagem
            let images = result.getattr(py, "images")?;
            let img = images.get_item(py, 0)?;

            // Converter para bytes
            let img_bytes = img.call_method0(py, "tobytes")?;
            Ok(img_bytes.extract(py)?)
        })
    }
}
```

2. **Adicionar depend√™ncias** (5 minutos)
```toml
[dependencies]
safetensors = "0.4"
pyo3 = { version = "0.21", features = ["auto-initialize"] }
```

3. **Instalar Python libs** (5 minutos)
```powershell
pip install torch torchvision diffusers transformers accelerate
```

4. **Modificar pipeline.rs** (1 hora)
```rust
pub enum DiffusionBackend {
    Native(UNet),        // Atual (r√°pido mas simples)
    Hybrid(HybridUNet),  // Novo (fotoreal√≠stico)
}

impl StableDiffusionPipeline {
    pub fn load_hybrid(model_path: &str) -> Result<Self, String> {
        let backend = DiffusionBackend::Hybrid(
            HybridUNet::load(model_path)?
        );
        // ... resto
    }
}
```

**Vantagens N√çVEL 2**:
- ‚úÖ Funciona IMEDIATAMENTE
- ‚úÖ Usa implementa√ß√£o madura (testada por milh√µes)
- ‚úÖ Compat√≠vel com TODOS os modelos
- ‚úÖ Mant√©m interface Rust (s√≥ backend √© Python)
- ‚úÖ Atualiza automaticamente (pip upgrade diffusers)

**Desvantagens**:
- ‚ö†Ô∏è Depende de Python runtime
- ‚ö†Ô∏è ~5GB de libs Python
- ‚ö†Ô∏è N√£o √© "100% Rust"

---

### N√çVEL 3: ONNX Runtime (Meio Termo) üîß
**Status**: N√ÉO INICIADO
**Esfor√ßo**: 1-2 semanas
**Capacidade**: Gera√ß√£o fotoreal√≠stica nativa

#### O que precisa:

1. **Exportar modelo para ONNX** (1 dia)
```python
# Script Python para converter
from diffusers import StableDiffusionPipeline
import torch

pipeline = StableDiffusionPipeline.from_single_file(
    "dreamshaper_8.safetensors"
)

# Exportar UNet
torch.onnx.export(
    pipeline.unet,
    (latents, timestep, encoder_hidden_states),
    "unet.onnx",
    input_names=["latents", "timestep", "encoder_hidden_states"],
    output_names=["noise_pred"],
    dynamic_axes={"latents": {0: "batch"}}
)

# Exportar VAE Decoder
torch.onnx.export(
    pipeline.vae.decoder,
    (latents,),
    "vae_decoder.onnx",
    input_names=["latents"],
    output_names=["sample"]
)

# Exportar Text Encoder (CLIP)
torch.onnx.export(
    pipeline.text_encoder,
    (input_ids,),
    "text_encoder.onnx",
    input_names=["input_ids"],
    output_names=["last_hidden_state"]
)
```

2. **Integrar ONNX Runtime** (3-5 dias)
```rust
use ort::{Environment, SessionBuilder, Value};

pub struct ONNXUNet {
    session: ort::Session,
}

impl ONNXUNet {
    pub fn load(onnx_path: &str) -> Result<Self, String> {
        let env = Environment::builder()
            .with_name("avila-diffusion")
            .build()?;

        let session = SessionBuilder::new(&env)?
            .with_model_from_file(onnx_path)?;

        Ok(Self { session })
    }

    pub fn forward(&self, latents: &[f32], timestep: f32, text_emb: &[f32])
        -> Result<Vec<f32>, String>
    {
        // Preparar inputs
        let latents_tensor = Value::from_array(
            self.session.allocator(),
            &[1, 4, 64, 64],
            latents
        )?;

        let timestep_tensor = Value::from_array(
            self.session.allocator(),
            &[1],
            &[timestep]
        )?;

        let text_tensor = Value::from_array(
            self.session.allocator(),
            &[1, 77, 768],
            text_emb
        )?;

        // Executar infer√™ncia
        let outputs = self.session.run(vec![
            latents_tensor,
            timestep_tensor,
            text_tensor
        ])?;

        // Extrair resultado
        let noise_pred = outputs[0].try_extract::<f32>()?;
        Ok(noise_pred.view().to_vec())
    }
}
```

3. **Adicionar depend√™ncia** (1 minuto)
```toml
[dependencies]
ort = "2.0"  # ONNX Runtime
```

4. **Download ONNX Runtime** (autom√°tico via cargo)

**Vantagens N√çVEL 3**:
- ‚úÖ 100% Rust (sem Python runtime)
- ‚úÖ Formato bin√°rio otimizado
- ‚úÖ Infer√™ncia r√°pida (TensorRT, DirectML)
- ‚úÖ Cross-platform
- ‚úÖ Menor consumo de mem√≥ria que PyTorch

**Desvantagens**:
- ‚ö†Ô∏è Precisa converter modelos primeiro
- ‚ö†Ô∏è ~500MB de runtime ONNX
- ‚ö†Ô∏è Menos flex√≠vel que PyTorch

---

### N√çVEL 4: Rust Puro (100% Nativo) üèÜ SONHO
**Status**: N√ÉO INICIADO
**Esfor√ßo**: 2-4 meses
**Capacidade**: Performance m√°xima, controle total

#### O que precisa implementar:

1. **Opera√ß√µes B√°sicas** (2-3 semanas)
   - [x] Tensor struct (j√° temos b√°sico)
   - [ ] Conv2D (convolu√ß√£o 2D)
   - [ ] BatchNorm2D
   - [ ] GroupNorm
   - [ ] Linear layers (fully connected)
   - [ ] Activations (SiLU, GELU, ReLU)
   - [ ] Upsampling/Downsampling
   - [ ] Autograd (opcional)

2. **Attention Mechanisms** (2 semanas)
   - [ ] Self-Attention (Q, K, V)
   - [ ] Cross-Attention (text conditioning)
   - [ ] Multi-head Attention
   - [ ] Flash Attention (otimiza√ß√£o)

3. **UNet Completo** (3-4 semanas)
   - [ ] ResNet Blocks
   - [ ] Attention Blocks
   - [ ] Down-sampling blocks (4 n√≠veis)
   - [ ] Middle block
   - [ ] Up-sampling blocks (4 n√≠veis)
   - [ ] Skip connections
   - [ ] Time embeddings (sinusoidal)
   - [ ] Total: ~350 layers

4. **VAE Completo** (2 semanas)
   - [ ] Encoder (4 down-blocks)
   - [ ] Latent distribution (mean, logvar)
   - [ ] Decoder (4 up-blocks)
   - [ ] KL divergence loss

5. **CLIP Completo** (2 semanas)
   - [ ] BPE Tokenizer (49,408 vocab)
   - [ ] Token embeddings (768D)
   - [ ] Positional embeddings
   - [ ] 12 Transformer layers
   - [ ] Layer normalization
   - [ ] Pooling

6. **Otimiza√ß√µes** (2-3 semanas)
   - [ ] SIMD (AVX2, NEON)
   - [ ] Multi-threading (rayon)
   - [ ] GPU kernels (wgpu/cuda)
   - [ ] FP16/BF16 support
   - [ ] Memory pooling
   - [ ] Fused operations

7. **Carregamento de Weights** (1 semana)
   - [x] Parse SafeTensors (j√° temos)
   - [ ] Reshape tensors para layers
   - [ ] Quantization (8-bit, 4-bit)
   - [ ] Weight freezing

**Linha de c√≥digo estimada**: ~15,000-20,000 linhas

**Exemplo de Conv2D nativo**:
```rust
pub struct Conv2D {
    weight: Tensor4D, // [out_channels, in_channels, kH, kW]
    bias: Option<Tensor1D>,
    stride: (usize, usize),
    padding: (usize, usize),
}

impl Conv2D {
    pub fn forward(&self, input: &Tensor4D) -> Tensor4D {
        let (batch, in_c, in_h, in_w) = input.shape();
        let (out_c, _, kh, kw) = self.weight.shape();

        let out_h = (in_h + 2 * self.padding.0 - kh) / self.stride.0 + 1;
        let out_w = (in_w + 2 * self.padding.1 - kw) / self.stride.1 + 1;

        let mut output = Tensor4D::zeros(batch, out_c, out_h, out_w);

        // Convolu√ß√£o (vers√£o simples, sem SIMD)
        for b in 0..batch {
            for oc in 0..out_c {
                for oh in 0..out_h {
                    for ow in 0..out_w {
                        let mut sum = 0.0;

                        // Kernel
                        for ic in 0..in_c {
                            for kh_i in 0..kh {
                                for kw_i in 0..kw {
                                    let ih = oh * self.stride.0 + kh_i - self.padding.0;
                                    let iw = ow * self.stride.1 + kw_i - self.padding.1;

                                    if ih < in_h && iw < in_w {
                                        sum += input[[b, ic, ih, iw]]
                                             * self.weight[[oc, ic, kh_i, kw_i]];
                                    }
                                }
                            }
                        }

                        if let Some(bias) = &self.bias {
                            sum += bias[oc];
                        }

                        output[[b, oc, oh, ow]] = sum;
                    }
                }
            }
        }

        output
    }
}
```

**Vantagens N√çVEL 4**:
- ‚úÖ 100% Rust, zero depend√™ncias Python
- ‚úÖ Performance m√°xima (com SIMD/GPU)
- ‚úÖ Controle total sobre otimiza√ß√µes
- ‚úÖ Bin√°rio final < 50MB
- ‚úÖ Deploy simples (single executable)
- ‚úÖ Integra√ß√£o perfeita com Avila Stack

**Desvantagens**:
- ‚ùå Trabalho massivo (3-4 meses)
- ‚ùå Debugging complexo
- ‚ùå Precisa expertise em deep learning
- ‚ùå Alto risco de bugs num√©ricos

---

## üìà COMPARA√á√ÉO DE PERFORMANCE

| M√©trica | N√≠vel 1 (Atual) | N√≠vel 2 (H√≠brido) | N√≠vel 3 (ONNX) | N√≠vel 4 (Rust) |
|---------|----------------|-------------------|----------------|----------------|
| **Tempo implementa√ß√£o** | ‚úÖ Pronto | 2-3 dias | 1-2 semanas | 2-4 meses |
| **Tempo gera√ß√£o (512x512)** | < 1s | 10-30s | 5-15s | 3-10s |
| **Qualidade** | Procedural | Fotoreal√≠stica ‚≠ê | Fotoreal√≠stica ‚≠ê | Fotoreal√≠stica ‚≠ê |
| **Consumo RAM** | 50MB | 4-8GB | 2-4GB | 1-3GB |
| **Tamanho bin√°rio** | 5MB | 15MB + 5GB Python | 500MB | 30-50MB |
| **Depende de Python** | ‚ùå | ‚úÖ | ‚ùå | ‚ùå |
| **GPU Acceleration** | ‚ùå | ‚úÖ Auto | ‚úÖ Auto | ‚ö†Ô∏è Manual |
| **Compatibilidade modelos** | N/A | 100% | 95% | 95% |
| **Manuten√ß√£o** | Baixa | Baixa | M√©dia | Alta |

---

## üéØ RECOMENDA√á√ÉO

### Para **Produ√ß√£o IMEDIATA** (esta semana):
üëâ **N√çVEL 2 (H√≠brido)**

**Raz√£o**:
- Funciona em 2-3 dias
- Gera√ß√£o fotoreal√≠stica garantida
- Usa os 8 modelos que voc√™ j√° tem
- Mant√©m sua interface Rust linda
- F√°cil de manter (pip upgrade)

### Para **Longo Prazo** (2026):
üëâ **N√çVEL 3 (ONNX)** ‚Üí **N√çVEL 4 (Rust Puro)**

**Raz√£o**:
- ONNX como ponte (elimina Python)
- Rust puro no final (soberania total)
- Integra√ß√£o com Avila Stack
- Performance m√°xima

---

## üìã CHECKLIST N√çVEL 2 (Pr√≥ximos Passos)

### Sprint 1 (Dia 1): Setup Python Integration
- [ ] Instalar PyO3: `cargo add pyo3 --features auto-initialize`
- [ ] Instalar diffusers: `pip install diffusers transformers torch`
- [ ] Criar m√≥dulo `src/hybrid_backend.rs`
- [ ] Implementar `HybridUNet::load()`
- [ ] Testar carregamento de 1 modelo

### Sprint 2 (Dia 2): Infer√™ncia
- [ ] Implementar `HybridUNet::forward()`
- [ ] Integrar no pipeline.rs
- [ ] Adicionar flag `--backend hybrid` no CLI
- [ ] Testar gera√ß√£o de 1 imagem
- [ ] Benchmark vs AUTOMATIC1111

### Sprint 3 (Dia 3): Polish & Deploy
- [ ] Adicionar cache de modelos
- [ ] Progress bar (20 steps)
- [ ] Error handling robusto
- [ ] Atualizar frontend (mostrar backend usado)
- [ ] Documenta√ß√£o completa
- [ ] Deploy servidor

---

## üí∞ CUSTO/BENEF√çCIO

### Op√ß√£o A: Contratar implementa√ß√£o N√çVEL 4
**Custo**: $30,000 - $80,000 USD
**Prazo**: 3-4 meses
**Risco**: Alto (bugs, delays)

### Op√ß√£o B: Fazer N√çVEL 2 agora
**Custo**: $0 (seu tempo: 2-3 dias)
**Prazo**: Esta semana
**Risco**: Baix√≠ssimo

### Op√ß√£o C: Usar AUTOMATIC1111 WebUI
**Custo**: $0
**Prazo**: J√° funciona
**Risco**: Zero
**Problema**: N√£o √© seu, n√£o tem branding

---

## üé® DEMONSTRA√á√ÉO DO QUE FALTA

### Atual (N√çVEL 1):
```
Prompt: "sunset over mountains"
Output: üü†üü°üî¥ (Gradiente laranja/vermelho procedural)
Tempo: 0.8s
```

### Com N√çVEL 2-4:
```
Prompt: "sunset over mountains, photorealistic, 8k"
Output: üèîÔ∏èüåÖ‚ú® (Foto real√≠stica com montanhas, nuvens, luz natural)
Tempo: 15s (N√çVEL 2)
```

---

## üöÄ DECIS√ÉO AGORA

**Qual caminho voc√™ escolhe?**

1. ‚ö° **R√ÅPIDO**: N√≠vel 2 (H√≠brido) - Funciona semana que vem
2. üéØ **BALANCEADO**: N√≠vel 3 (ONNX) - 2 semanas
3. üèÜ **COMPLETO**: N√≠vel 4 (Rust Puro) - 3 meses
4. üí§ **ESPERAR**: Ficar no N√≠vel 1 (procedural)

Eu **fortemente recomendo** come√ßar com **N√≠vel 2** enquanto planeja N√≠vel 4 para 2026.

Voc√™ ter√°:
- ‚úÖ Produto funcionando J√Å
- ‚úÖ Usu√°rios testando
- ‚úÖ Feedback real
- ‚úÖ Tempo para planejar Rust puro

**Quer que eu implemente o N√çVEL 2 agora?** üéØ
